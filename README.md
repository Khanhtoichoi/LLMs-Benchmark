# LLMs-Benchmark
A list of benchmarks dataset with purpose of evaluating LLMs based on coding task. 
# Benchmark for evaluation LLMs

# **1. HumanEval**

HumanEval is the dataset of hand-written problems, developed by OpenAI for purpose of evaluate of LLMs in writing code, particularly in generating functions based on prompts.

- **Structure:** Consists of total 164 programming problems, where each issue includes a function signature, docstring, body, and several unit tests with average of 7.7 tests per problem.
    
    Input: Code generated by your LLMs through a set of defined prompts 
    
    Output: Points on each problem through their unit test.
    
- **Target:** Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms and simple mathematics. We can evaluate the functional correctness and measure the problem-solving capabilities of LLMs.
- **Example:**

![Uploading image.pngâ€¦]()

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/4dde8055-036b-48fd-a339-91d382b2d165/image.png)

**Paper related:** https://arxiv.org/pdf/2107.03374#page=16&zoom=100,0,0

**GitHub:** https://github.com/openai/human-eval

# **2. CodeBERTScore**

CodeBERTScore is an Automatic Evaluation Metric for code-generation developed by NeuLab, leveraging self-supervised pretrained models of code such as CodeBERT

- **Structure:** CodeBERTScore encodes the generated code and the reference code independently with pretrained  models, with the instructions of natural language. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall.
    
    Input: Generated code by LLMs
    
    Output: Average score 
    
- **Target:** CodeBERTScore evaluate the quality of code against ground truth code. This helps to determine the accuracy and effectiveness of the generated code.
- **Example:**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/f0cf21d0-88c2-442f-ae67-eccb8cba37f7/image.png)

**Paper related:** https://arxiv.org/pdf/2302.05527

**GitHub:** https://github.com/neulab/code-bert-score

# **3. CodeXGLUE**

CodeXGLUE (General Language Understanding Evaluation benchmark for CODE) is a benchmark dataset and open challenge for code intelligence

- **Structure:**  includes 14 datasets for 10 diversified programming language understanding and generation tasks, and a platform for model evaluation and comparison. I
- **Target:** To assess code intelligence, including understanding, fixing, and explaining code. This would be very necessary in code analysis and software development.
- **Example:**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/19159416-d4a8-445b-acc8-1ac7db4ec8e4/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/945c2fbf-4300-4b32-9f56-e09a985d2681/image.png)

**Paper related:** https://arxiv.org/pdf/2102.04664

**GitHub:** https://github.com/microsoft/CodeXGLUE

# 4. Mostly Basic Python Programming (MBPP)

MBPP is a benchmark dataset focus on basic to intermediate-level Python programming tasks.

- **Structure:** Includes 1000 Python Programming problems suitable for entry-level programmers.
- **Target:** To evaluate proficiency in solving basic programming tasks and understanding of Python of LLMs
- **Example:**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/722ecb35-6076-4ad6-8e30-2aecb5c9680f/image.png)

- **Paper related:** https://arxiv.org/pdf/2108.07732
- **Hugging Face dataset:**  [MBPP Dataset: HuggingFace](https://huggingface.co/datasets/mbpp)

# 5. Class-Eval

ClassEval is the first class-level code generation benchmark.

- **Structure:** We manually build ClassEval of 100 class-level Python coding tasks, consists of 100 classes and 410 methods, and average 33.1 test cases per class.
- **Target:** Evaluating LLMs on Class-level code generation
- **Example:**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/ce394261-f619-476b-9179-9894dbd205ce/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/74856217-6a43-4a2b-92f7-0b739d4f8efb/image.png)

- **Paper related:** https://arxiv.org/pdf/2308.01861
- **GitHub:** https://github.com/FudanSELab/ClassEval?tab=readme-ov-file

# 6. CodeUltraFeedback:

CodeUltraFeedback-Bench is a benchmark to evaluate and compare LLM alignment using LLM-as-a-Judge over five coding preferences: instruction following, code explanation, code complexity and efficiency, code readability, and coding style.

- **Structure:**  a preference dataset of 10,000 complex instructions and 40,000 responses generated using 14 diverse LLMs for aligning LLMs to coding preferences in a code generation scenario.
- **Example:**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/eddd18f4-8c7e-43c2-9af2-03b6222c039b/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/6b9dbbde-ae44-4087-bd85-fbfabdc0d6dd/image.png)

- **Paper related:** https://arxiv.org/pdf/2403.09032
- **GitHub:** https://github.com/martin-wey/CodeUltraFeedback

# 7. PythonSaga

PythonSaga is a new Python code generation benchmark that addresses the limitations of existing benchmarks with respect to diversity in concepts and difficulty level.

- **Structure:** contains 185 prompts, close to equal representation from each of the 38 programming concepts with varied levels of difficulty

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/12a72466-d1d9-4c53-9d23-dea80619583f/1ef61a8f-f2ea-4b63-9ef4-def9627c2bf3/image.png)

- **Paper related:** https://arxiv.org/html/2401.03855v2#A1.SS2
- **Source Code:** https://anonymous.4open.science/r/PythonSaga/DataSet/sample_output.jsonl
